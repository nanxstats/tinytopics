{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"tinytopics","text":"<p>Topic modeling via sum-to-one constrained neural Poisson NMF. Built with PyTorch, runs on both CPUs and GPUs.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#using-pip","title":"Using pip","text":"<p>You can install tinytopics from PyPI:</p> <pre><code>pip install tinytopics\n</code></pre> <p>Or install the development version from GitHub:</p> <pre><code>git clone https://github.com/nanxstats/tinytopics.git\ncd tinytopics\npython3 -m pip install -e .\n</code></pre>"},{"location":"#install-pytorch-with-gpu-support","title":"Install PyTorch with GPU support","text":"<p>The above will likely install the CPU version of PyTorch by default. To install PyTorch with GPU support, follow the official guide.</p> <p>For example, install PyTorch for Windows with CUDA 12.6:</p> <pre><code>pip uninstall torch\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n</code></pre>"},{"location":"#install-alternative-pytorch-versions","title":"Install alternative PyTorch versions","text":"<p>For users stuck with older PyTorch or NumPy versions, for instance, in HPC cluster settings, a workaround is to skip installing the dependencies with <code>--no-deps</code> and install specific versions of all dependencies manually:</p> <pre><code>pip install tinytopics --no-deps\npip install torch==2.2.0\n</code></pre>"},{"location":"#use-tinytopics-in-a-project","title":"Use tinytopics in a project","text":"<p>To have a more hassle-free package management experience, it is recommended to use tinytopics as a dependency under a project context using virtual environments.</p> <p>You should probably set up a manual source/index for PyTorch. As examples, check out the official guidelines when using Rye or using uv.</p>"},{"location":"#examples","title":"Examples","text":"<p>After tinytopics is installed, try examples from:</p> <ul> <li>Getting started guide with simulated count data</li> <li>CPU vs. GPU speed benchmark</li> <li>Text data topic modeling example</li> <li>Memory-efficient training</li> <li>Distributed training</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#tinytopics-091","title":"tinytopics 0.9.1","text":""},{"location":"changelog/#maintenance","title":"Maintenance","text":"<ul> <li>Added GitHub Actions workflow to run <code>ruff check</code> for code linting (#71).</li> <li>Updated GitHub Actions workflows to use <code>actions/checkout@v6</code> (#71).</li> <li>Updated badges in <code>README.md</code> (#71).</li> </ul>"},{"location":"changelog/#tinytopics-090","title":"tinytopics 0.9.0","text":""},{"location":"changelog/#improvements","title":"Improvements","text":"<ul> <li>Added Python 3.14 support by conditionally requiring torch &gt;= 2.9.0 under   Python 3.14 (#67).</li> <li>Removed <code>pyreadr</code> dependency and replaced with <code>safetensors</code> for safer   unserialization and better forward compatibility with Python versions (#66).</li> </ul>"},{"location":"changelog/#tinytopics-081","title":"tinytopics 0.8.1","text":""},{"location":"changelog/#linting","title":"Linting","text":"<ul> <li>Added ruff linter configuration to <code>pyproject.toml</code> with popular rule sets   including pycodestyle, Pyflakes, pyupgrade, flake8-bugbear, flake8-simplify,   and isort (#63).</li> <li>Fixed <code>ruff check</code> linting issues such as PEP 585, unused imports/variables,   Yoda conditions, and long lines (#63).</li> </ul>"},{"location":"changelog/#maintenance_1","title":"Maintenance","text":"<ul> <li>Use Python 3.13.8 in default development environment (#62).</li> <li>Updated GitHub Actions workflows to use the latest <code>checkout</code> and   <code>setup-python</code> versions (#62).</li> <li>Refactored the logo generation script to use ImageMagick, removing the   previous R and hexSticker dependency (#61).</li> </ul>"},{"location":"changelog/#tinytopics-080","title":"tinytopics 0.8.0","text":""},{"location":"changelog/#typing","title":"Typing","text":"<ul> <li>Add mypy as a development dependency and fix all mypy type checking issues (#56).</li> </ul>"},{"location":"changelog/#maintenance_2","title":"Maintenance","text":"<ul> <li>Add a GitHub Actions workflow to run mypy checks and a mypy badge to <code>README.md</code> (#57).</li> </ul>"},{"location":"changelog/#tinytopics-075","title":"tinytopics 0.7.5","text":""},{"location":"changelog/#maintenance_3","title":"Maintenance","text":"<ul> <li>Removed download statistics badge from <code>README.md</code> due to availability issues   with the service (#52).</li> <li>Use Python 3.13.7 for the default package development environment (#53).</li> </ul>"},{"location":"changelog/#tinytopics-074","title":"tinytopics 0.7.4","text":""},{"location":"changelog/#improvements_1","title":"Improvements","text":"<ul> <li>Add Python 3.13 support by conditionally requiring torch &gt;= 2.6.0 under   Python &gt;= 3.13 (#47).</li> </ul>"},{"location":"changelog/#documentation","title":"Documentation","text":"<ul> <li>Extend the installation section in <code>README.md</code> to explain the use cases   on GPU support, dependency override, and project dependency management (#48).</li> </ul>"},{"location":"changelog/#maintenance_4","title":"Maintenance","text":"<ul> <li>Manage project with uv (#46).</li> <li>Change logo typeface for a fresh look. Improve the logo text rendering   workflow to use SVG (#45).</li> <li>Change logo image path from relative to absolute URL for proper rendering   on PyPI (#44).</li> </ul>"},{"location":"changelog/#tinytopics-073","title":"tinytopics 0.7.3","text":""},{"location":"changelog/#maintenance_5","title":"Maintenance","text":"<ul> <li>Use <code>.yml</code> extension for GitHub Actions workflows consistently (#40).</li> <li>Use isort and ruff to sort imports and format Python code (#41).</li> </ul>"},{"location":"changelog/#tinytopics-072","title":"tinytopics 0.7.2","text":""},{"location":"changelog/#new-features","title":"New features","text":"<ul> <li>Add <code>TorchDiskDataset</code> class to support using <code>.pt</code> or <code>.pth</code> files   as inputs for <code>fit_model()</code> and <code>fit_model_distributed()</code> (#38).   Similar to <code>NumpyDiskDataset</code> added in tinytopics 0.6.0, this class also   uses memory-mapped mode to load data so that larger than system memory   datasets can be used for training.</li> </ul>"},{"location":"changelog/#tinytopics-071","title":"tinytopics 0.7.1","text":""},{"location":"changelog/#documentation_1","title":"Documentation","text":"<ul> <li>Add distributed training speed and cost metrics on 8x A100 (40 GB SXM4) to   the distributed training   article (#34). This supplements the existing 1x H100 and 4x H100 metrics.</li> </ul>"},{"location":"changelog/#testing","title":"Testing","text":"<ul> <li>Add unit tests for <code>fit_model_distributed()</code> (#35).</li> <li>Add pytest-cov to development dependencies (#35).</li> </ul>"},{"location":"changelog/#tinytopics-070","title":"tinytopics 0.7.0","text":""},{"location":"changelog/#new-features_1","title":"New features","text":"<ul> <li>Add <code>fit_model_distributed()</code> to support distributed training using   Hugging Face Accelerate.   See the distributed training   article for details (#32).</li> </ul>"},{"location":"changelog/#improvements_2","title":"Improvements","text":"<ul> <li>Use <code>tqdm.auto</code> for better progress bar visuals when used in notebooks (#30).</li> <li>Move dataset classes and loss functions into dedicated modules to improve   code structure and reusability (#31).</li> </ul>"},{"location":"changelog/#tinytopics-060","title":"tinytopics 0.6.0","text":""},{"location":"changelog/#new-features_2","title":"New features","text":"<ul> <li><code>fit_model()</code> now supports using PyTorch <code>Dataset</code> as input, in addition   to in-memory tensors. This allows fitting topic models on data larger than   GPU VRAM or system RAM. The <code>NumpyDiskDataset</code> class is added to read   <code>.npy</code> document-term matrices from disk on-demand (#26).</li> </ul>"},{"location":"changelog/#documentation_2","title":"Documentation","text":"<ul> <li>Added a memory-efficient training   article demonstrating the new features for fitting topic models on   large datasets (#27).</li> </ul>"},{"location":"changelog/#tinytopics-051","title":"tinytopics 0.5.1","text":""},{"location":"changelog/#documentation_3","title":"Documentation","text":"<ul> <li>Add badges for CI tests and mkdocs workflows to <code>README.md</code> (#24).</li> <li>Add PyTorch management guide link for uv to <code>README.md</code> (735fcca).</li> </ul>"},{"location":"changelog/#maintenance_6","title":"Maintenance","text":"<ul> <li>Use hatchling 1.26.3 in <code>pyproject.toml</code> to work around <code>rye publish</code>   errors (c56387c).</li> </ul>"},{"location":"changelog/#tinytopics-050","title":"tinytopics 0.5.0","text":""},{"location":"changelog/#improvements_3","title":"Improvements","text":"<ul> <li> <p>Increased the speed of <code>generate_synthetic_data()</code> significantly by using   direct mixture sampling, which leverages the properties of multinomial   distributions (#21).</p> <p>This change makes simulating data at the scale of 100K x 100K more feasible. Although the approaches before and after are mathematically equivalent, the data generated with the same seed in previous versions and this version onward will be bitwise different.</p> </li> </ul>"},{"location":"changelog/#tinytopics-041","title":"tinytopics 0.4.1","text":""},{"location":"changelog/#documentation_4","title":"Documentation","text":"<ul> <li>Use <code>pip</code> and <code>python3</code> in command line instructions consistently.</li> </ul>"},{"location":"changelog/#tinytopics-040","title":"tinytopics 0.4.0","text":""},{"location":"changelog/#breaking-changes","title":"Breaking changes","text":"<ul> <li>tinytopics now requires Python &gt;= 3.10 to use PEP 604 style shorthand syntax   for union and optional types (#14).</li> </ul>"},{"location":"changelog/#typing_1","title":"Typing","text":"<ul> <li>Refactor type hints to use more base abstract classes, making them less   limiting to specific implementations (#14).</li> </ul>"},{"location":"changelog/#testing_1","title":"Testing","text":"<ul> <li>Add unit tests for all functions using pytest, with a GitHub Actions workflow   to run tests under Linux and Windows (#18).</li> </ul>"},{"location":"changelog/#improvements_4","title":"Improvements","text":"<ul> <li>Update articles to simplify import syntax using <code>import tinytopics as tt</code> (#16).</li> <li>Close precise figure handles in plot functions instead of the current figure (#18).</li> </ul>"},{"location":"changelog/#bug-fixes","title":"Bug fixes","text":"<ul> <li>Plot functions now correctly use string and list type color palette inputs   when specified (do not call them as functions) (#18).</li> </ul>"},{"location":"changelog/#tinytopics-030","title":"tinytopics 0.3.0","text":""},{"location":"changelog/#improvements_5","title":"Improvements","text":"<ul> <li>Refactor the code to use a more functional style and add type hints   to improve code clarity (#9).</li> </ul>"},{"location":"changelog/#tinytopics-020","title":"tinytopics 0.2.0","text":""},{"location":"changelog/#new-features_3","title":"New features","text":"<ul> <li>Add <code>scale_color_tinytopics()</code> to support the coloring need for   arbitrary number of topics (#4).</li> </ul>"},{"location":"changelog/#improvements_6","title":"Improvements","text":"<ul> <li>Simplify hyperparameter tuning by adopting modern stochastic gradient methods.   <code>fit_model()</code> now uses a combination of the AdamW optimizer (with weight   decay) and the cosine annealing (with warm restarts) scheduler (#2).</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"Bug fixes","text":"<ul> <li>Fix \"Structure plot\" y-axis range issue by adding a <code>normalize_rows</code> argument   to <code>plot_structure()</code> for normalizing rows so that they all sum exactly to 1,   and explicitly setting the y-axis limit to [0, 1]. (#1).</li> </ul>"},{"location":"changelog/#documentation_5","title":"Documentation","text":"<ul> <li>Add text data topic modeling example article (#7).</li> </ul>"},{"location":"changelog/#tinytopics-013","title":"tinytopics 0.1.3","text":""},{"location":"changelog/#improvements_7","title":"Improvements","text":"<ul> <li>Reorder arguments in plotting functions to follow conventions.</li> </ul>"},{"location":"changelog/#tinytopics-012","title":"tinytopics 0.1.2","text":""},{"location":"changelog/#improvements_8","title":"Improvements","text":"<ul> <li>Reduce the minimum version requirement for all dependencies in <code>pyproject.toml</code>.</li> </ul>"},{"location":"changelog/#documentation_6","title":"Documentation","text":"<ul> <li>Add more details on PyTorch installation in <code>README.md</code>.</li> <li>Improve text quality in articles.</li> </ul>"},{"location":"changelog/#tinytopics-011","title":"tinytopics 0.1.1","text":""},{"location":"changelog/#improvements_9","title":"Improvements","text":"<ul> <li>Add <code>CHANGELOG.md</code> to record changes.</li> <li>Add essential metadata to <code>pyproject.toml</code>.</li> </ul>"},{"location":"changelog/#tinytopics-010","title":"tinytopics 0.1.0","text":""},{"location":"changelog/#new-features_4","title":"New features","text":"<ul> <li>First version.</li> </ul>"},{"location":"articles/benchmark/","title":"CPU vs.\u00a0GPU benchmark","text":"<p>Tip</p> <p>To run the code from this article as a Python script:</p> <pre><code>python3 examples/benchmark.py\n</code></pre> <p>Let\u2019s evaluate the tinytopics topic model training speed on CPU vs.\u00a0GPU on mainstream consumer hardware using simulated data. We will compare the time consumed under combinations of the three key parameters defining the problem size:</p> <ul> <li>Number of documents (<code>n</code>).</li> <li>Number of terms or vocabulary size (<code>m</code>).</li> <li>Number of topics (<code>k</code>).</li> </ul> <p>Experiment environment:</p> <ul> <li>GPU: 1x NVIDIA GeForce RTX 4090 (16384 CUDA cores, 24GB VRAM)</li> <li>CPU: 1x AMD Ryzen 9 7950X3D (16 cores, 32 threads)</li> <li>RAM: DDR5 6000MHz 2x32GB</li> </ul>"},{"location":"articles/benchmark/#conclusions","title":"Conclusions","text":"<ul> <li>Training time grows linearly as the number of documents (<code>n</code>) grows,   on both CPU and GPU.</li> <li>Similarly, training time grows as the number of topics (<code>k</code>) grows.</li> <li>With <code>n</code> and <code>k</code> fixed and vocabulary size (<code>m</code>) grows, CPU time will   grow linearly while GPU time stays constant. For <code>m</code> larger than a   certain threshold, training on GPU will be faster than CPU.</li> </ul>"},{"location":"articles/benchmark/#import-tinytopics","title":"Import tinytopics","text":"<pre><code>import time\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\nimport tinytopics as tt\n</code></pre>"},{"location":"articles/benchmark/#basic-setup","title":"Basic setup","text":"<p>Set seed for reproducibility:</p> <pre><code>tt.set_random_seed(42)\n</code></pre> <p>Define parameter grids:</p> <pre><code>n_values = [1000, 5000]  # Number of documents\nm_values = [1000, 5000, 10000, 20000]  # Vocabulary size\nk_values = [10, 50, 100]  # Number of topics\navg_doc_length = 256 * 256\n</code></pre> <p>Create a data frame to store the benchmark results.</p> <pre><code>benchmark_results = pd.DataFrame()\n\ndef benchmark(X, k, device):\n    start_time = time.time()\n    model, losses = tt.fit_model(X, k, device=device)\n    elapsed_time = time.time() - start_time\n\n    return elapsed_time\n</code></pre>"},{"location":"articles/benchmark/#run-experiment","title":"Run experiment","text":"<pre><code>for n in n_values:\n    for m in m_values:\n        for k in k_values:\n            print(f\"Benchmarking for n={n}, m={m}, k={k}...\")\n\n            X, true_L, true_F = tt.generate_synthetic_data(n, m, k, avg_doc_length=avg_doc_length)\n\n            # Benchmark on CPU\n            cpu_time = benchmark(X, k, torch.device(\"cpu\"))\n            cpu_result = pd.DataFrame([{\"n\": n, \"m\": m, \"k\": k, \"device\": \"CPU\", \"time\": cpu_time}])\n\n            if not cpu_result.isna().all().any():\n                benchmark_results = pd.concat([benchmark_results, cpu_result], ignore_index=True)\n\n            # Benchmark on GPU if available\n            if torch.cuda.is_available():\n                gpu_time = benchmark(X, k, torch.device(\"cuda\"))\n                gpu_result = pd.DataFrame([{\"n\": n, \"m\": m, \"k\": k, \"device\": \"GPU\", \"time\": gpu_time}])\n\n                if not gpu_result.isna().all().any():\n                    benchmark_results = pd.concat([benchmark_results, gpu_result], ignore_index=True)\n</code></pre> <p>Save results to a CSV file:</p> <pre><code>benchmark_results.to_csv(\"benchmark-results.csv\", index=False)\n</code></pre>"},{"location":"articles/benchmark/#visualize-results","title":"Visualize results","text":"<p>Plot the number of terms (<code>m</code>) against the time consumed, conditioning on the number of documents (<code>n</code>), for each number of topics (<code>k</code>).</p> <pre><code>unique_series = len(n_values) * (2 if torch.cuda.is_available() else 1)\ncolormap = tt.scale_color_tinytopics(unique_series)\ncolors_list = [colormap(i) for i in range(unique_series)]\n\nfor k in k_values:\n    plt.figure(figsize=(7, 4.3), dpi=300)\n\n    color_idx = 0\n    for n in n_values:\n        subset = benchmark_results[\n            (benchmark_results[\"n\"] == n) &amp; (benchmark_results[\"k\"] == k)\n        ]\n\n        # Plot CPU results with a specific color\n        plt.plot(\n            subset[subset[\"device\"] == \"CPU\"][\"m\"],\n            subset[subset[\"device\"] == \"CPU\"][\"time\"],\n            label=f\"CPU (n={n})\",\n            linestyle=\"--\",\n            marker=\"o\",\n            color=colors_list[color_idx],\n        )\n        color_idx += 1\n\n        # Plot GPU results if available\n        if torch.cuda.is_available():\n            plt.plot(\n                subset[subset[\"device\"] == \"GPU\"][\"m\"],\n                subset[subset[\"device\"] == \"GPU\"][\"time\"],\n                label=f\"GPU (n={n})\",\n                linestyle=\"-\",\n                marker=\"x\",\n                color=colors_list[color_idx],\n            )\n            color_idx += 1\n\n    plt.xlabel(\"Vocabulary size (m)\")\n    plt.ylabel(\"Training time (seconds)\")\n    plt.title(f\"Training time vs. vocabulary size (k={k})\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(f\"training-time-k-{k}.png\", dpi=300)\n    plt.close()\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"articles/distributed/","title":"Distributed training","text":""},{"location":"articles/distributed/#overview","title":"Overview","text":"<p>tinytopics &gt;= 0.7.0 supports distributed training using Hugging Face Accelerate. This article demonstrates how to run distributed training on a single node with multiple GPUs.</p> <p>The example utilizes Distributed Data Parallel (DDP) for distributed training. This approach assumes that the model parameters fit within the memory of a single GPU, as each GPU maintains a synchronized copy of the model. The input data can exceed the memory capacity. This is generally a reasonable assumption for topic modeling tasks, as storing the factorized matrices is often less memory-intensive.</p> <p>Hugging Face Accelerate also supports other distributed training strategies such as Fully Sharded Data Parallel (FSDP) and DeepSpeed, which distribute model tensors across different GPUs and allow training larger models at the cost of speed.</p>"},{"location":"articles/distributed/#generate-data","title":"Generate data","text":"<p>We will use a 100k x 100k count matrix with 20 topics for distributed training. To generate the example data, save the following code to <code>distributed_data.py</code> and run:</p> <pre><code>python distributed_data.py\n</code></pre> <pre><code>import os\n\nimport numpy as np\n\nimport tinytopics as tt\n\n\ndef main():\n    n, m, k = 100_000, 100_000, 20\n    data_path = \"X.npy\"\n\n    if os.path.exists(data_path):\n        print(f\"Data already exists at {data_path}\")\n        return\n\n    print(\"Generating synthetic data...\")\n    tt.set_random_seed(42)\n    X, true_L, true_F = tt.generate_synthetic_data(\n        n=n, m=m, k=k, avg_doc_length=256 * 256\n    )\n\n    print(f\"Saving data to {data_path}\")\n    X_numpy = X.cpu().numpy()\n    np.save(data_path, X_numpy)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Generating the data is time-consuming (about 10 minutes), so running it as a standalone script helps avoid potential timeout errors during distributed training. You can also execute it on an instance type suitable for your data ingestion pipeline, rather than using valuable GPU instance hours.</p>"},{"location":"articles/distributed/#run-distributed-training","title":"Run distributed training","text":"<p>First, configure the distributed environment by running:</p> <pre><code>accelerate config\n</code></pre> <p>You will be prompted to answer questions about the distributed training environment and strategy. The answers will be saved to a configuration file at:</p> <pre><code>~/.cache/huggingface/accelerate/default_config.yaml\n</code></pre> <p>You can rerun <code>accelerate config</code> at any time to update the configuration. For data distributed parallel on a 4-GPU node, select single-node multi-GPU training options with the number of GPUs set to 4, and use the default settings for the remaining questions (mostly \"no\").</p> <p>Next, save the following code to <code>distributed_training.py</code> and run:</p> <pre><code>accelerate launch distributed_training.py\n</code></pre> <pre><code>import os\n\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\n\nimport tinytopics as tt\n\n\ndef main():\n    accelerator = Accelerator()\n    set_seed(42)\n    k = 20\n    data_path = \"X.npy\"\n\n    if not os.path.exists(data_path):\n        raise FileNotFoundError(\n            f\"{data_path} not found. Run distributed_data.py first.\"\n        )\n\n    print(f\"Loading data from {data_path}\")\n    X = tt.NumpyDiskDataset(data_path)\n\n    # All processes should have the data before proceeding\n    accelerator.wait_for_everyone()\n\n    model, losses = tt.fit_model_distributed(X, k=k)\n\n    # Only the main process should plot the loss\n    if accelerator.is_main_process:\n        tt.plot_loss(losses, output_file=\"loss.png\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>This script uses <code>fit_model_distributed()</code> (added in tinytopics 0.7.0) to train the model. Since distributed training on large datasets likely takes longer, <code>fit_model_distributed()</code> displays more detailed progress bars for each epoch, going through all batches in each epoch.</p>"},{"location":"articles/distributed/#sample-runs","title":"Sample runs","text":"<p>We ran the distributed training example on a 1-GPU, 4-GPU, and 8-GPU setup with H100 and A100 GPUs. The table below shows the training time per epoch, total time, GPU utilization, VRAM usage, instance cost, and total cost.</p> Metric 1x H100 (80 GB SXM5) 4x H100 (80 GB SXM5) 8x A100 (40 GB SXM4) Time per epoch (s) 24 6 6 Total time (min) 80 20 20 GPU utilization 16% 30-40% 30-50% VRAM usage 1% 4% 4% Instance cost (USD/h) 3.29 12.36 10.32 Total cost (USD) 4.38 4.12 3.44 <p>Using 4 H100 GPUs is approximately 4x faster than using 1 H100 GPU, with a slightly lower total cost. Using 8x A100 GPUs has similar speed comparing to 4x H100 GPUs but with an even lower total cost due to the lower instance cost.</p> <p>The loss plot and real-time GPU utilization monitoring via <code>nvtop</code> on the 4x H100 GPU instance are shown below.</p> <p></p> <p></p> <p>For more technical details on distributed training, please refer to the Hugging Face Accelerate documentation, as this article covers only the basics.</p>"},{"location":"articles/get-started/","title":"Get started","text":"<p>Tip</p> <p>To run the code from this article as a Python script:</p> <pre><code>python3 examples/get-started.py\n</code></pre> <p>Let\u2019s walk through a canonical tinytopics workflow using a synthetic dataset.</p>"},{"location":"articles/get-started/#import-tinytopics","title":"Import tinytopics","text":"<pre><code>import tinytopics as tt\n</code></pre>"},{"location":"articles/get-started/#generate-synthetic-data","title":"Generate synthetic data","text":"<p>Set random seed for reproducibility:</p> <pre><code>tt.set_random_seed(42)\n</code></pre> <p>Generate a synthetic dataset:</p> <pre><code>n, m, k = 5000, 1000, 10\nX, true_L, true_F = tt.generate_synthetic_data(n, m, k, avg_doc_length=256 * 256)\n</code></pre>"},{"location":"articles/get-started/#fit-topic-model","title":"Fit topic model","text":"<p>Fit the topic model and plot the loss curve. There will be a progress bar.</p> <pre><code>model, losses = tt.fit_model(X, k)\n\ntt.plot_loss(losses, output_file=\"loss.png\")\n</code></pre> <p></p> <p>Tip</p> <p>By default, tinytopics uses AdamW with weight decay as the optimizer, and the cosine annealing with warm restarts scheduler. This combination should help reduce the need of extensive manual tuning of hyperparameters such as the learning rate. For optimal performance, exploring the possible tuning parameter space is still recommended.</p>"},{"location":"articles/get-started/#post-process-results","title":"Post-process results","text":"<p>Get the learned L and F matrices from the fitted topic model:</p> <pre><code>learned_L = model.get_normalized_L().numpy()\nlearned_F = model.get_normalized_F().numpy()\n</code></pre> <p>To make it easier to inspect the results visually, we should try to \u201calign\u201d the learned topics with the ground truth topics by their terms similarity.</p> <pre><code>aligned_indices = tt.align_topics(true_F, learned_F)\nlearned_F_aligned = learned_F[aligned_indices]\nlearned_L_aligned = learned_L[:, aligned_indices]\n</code></pre> <p>Sort the documents in both the true document-topic matrix and the learned document-topic matrix, grouped by dominant topics.</p> <pre><code>sorted_indices = tt.sort_documents(true_L)\ntrue_L_sorted = true_L[sorted_indices]\nlearned_L_sorted = learned_L_aligned[sorted_indices]\n</code></pre> <p>Note</p> <p>The alignment step mostly only applies to simulation studies because we often don't know the ground truth L and F for real datasets.</p>"},{"location":"articles/get-started/#visualize-results","title":"Visualize results","text":"<p>We can use a \u201cStructure plot\u201d to visualize and compare the document-topic distributions.</p> <pre><code>tt.plot_structure(\n    true_L_sorted,\n    normalize_rows=True,\n    title=\"True document-topic distributions (sorted)\",\n    output_file=\"L-true.png\",\n)\n</code></pre> <p></p> <pre><code>tt.plot_structure(\n    learned_L_sorted,\n    normalize_rows=True,\n    title=\"Learned document-topic distributions (sorted and aligned)\",\n    output_file=\"L-learned.png\",\n)\n</code></pre> <p></p> <p>We can also plot the top terms for each topic using bar charts.</p> <pre><code>tt.plot_top_terms(\n    true_F,\n    n_top_terms=15,\n    title=\"Top terms per topic - true F matrix\",\n    output_file=\"F-top-terms-true.png\",\n)\n</code></pre> <p></p> <pre><code>tt.plot_top_terms(\n    learned_F_aligned,\n    n_top_terms=15,\n    title=\"Top terms per topic - learned F matrix (aligned)\",\n    output_file=\"F-top-terms-learned.png\",\n)\n</code></pre> <p></p>"},{"location":"articles/memory/","title":"Memory-efficient training","text":"<p>Tip</p> <p>To run the code from this article as a Python script:</p> <pre><code>python3 examples/memory.py\n</code></pre> <p>This article discusses solutions for training topic models on datasets larger than the available GPU VRAM or system RAM.</p>"},{"location":"articles/memory/#training-data-larger-than-vram-but-smaller-than-ram","title":"Training data larger than VRAM but smaller than RAM","text":"<p>This scenario is manageable. Let\u2019s see an example. We simulate a 100k x 100k dataset, requiring 37GB of memory. In this test, the dataset is larger than the 24GB VRAM but smaller than the 64GB system RAM.</p> <pre><code>import tinytopics as tt\n\ntt.set_random_seed(42)\n\nn, m, k = 100_000, 100_000, 20\nX, true_L, true_F = tt.generate_synthetic_data(n, m, k, avg_doc_length=256 * 256)\n\nsize_gb = X.nbytes / (1024**3)\nprint(f\"Memory size of X: {size_gb:.2f} GB\")\n\nmodel, losses = tt.fit_model(X, k=k, num_epochs=200)\n\ntt.plot_loss(losses, output_file=\"loss.png\")\n</code></pre> <p></p> <p>Each epoch takes around 6 seconds. The peak GPU VRAM usage is 23.5GB, and the peak RAM usage is around 60GB.</p> <p>Although the full dataset requires ~40 GB of RAM, the training process only moves one small batch (controlled by <code>batch_size</code> in <code>fit_model()</code>) onto the GPU at a time. The model parameters and a batch of data fit within the 24GB VRAM, allowing the training to proceed.</p>"},{"location":"articles/memory/#stream-training-data-from-disk","title":"Stream training data from disk","text":"<p>A more general solution in PyTorch is to use map-style and iterable-style datasets to stream data from disk on-demand, without loading the entire tensor into system memory.</p> <p>You can use the <code>NumpyDiskDataset</code> or <code>TorchDiskDataset</code> classes to load <code>.npy</code> or <code>.pt</code> datasets from disk as training data, supported by both <code>fit_model()</code> and <code>fit_model_distributed()</code>. Here is an example:</p> <pre><code>import numpy as np\n\nimport tinytopics as tt\n\ntt.set_random_seed(42)\n\nn, m, k = 100_000, 100_000, 20\nX, true_L, true_F = tt.generate_synthetic_data(n, m, k, avg_doc_length=256 * 256)\n\nsize_gb = X.nbytes / (1024**3)\nprint(f\"Memory size of X: {size_gb:.2f} GB\")\n\ndata_path = \"X.npy\"\nnp.save(data_path, X.cpu().numpy())\n\ndel X, true_L, true_F\n\ndataset = tt.NumpyDiskDataset(data_path)\nmodel, losses = tt.fit_model(dataset, k=k, num_epochs=100)\n\ntt.plot_loss(losses, output_file=\"loss.png\")\n</code></pre>"},{"location":"articles/memory/#training-data-larger-than-ram","title":"Training data larger than RAM","text":"<p>Let\u2019s demonstrate using a dataset larger than RAM. We will sample the rows of the previous 100k x 100k dataset to construct a 500k x 100k dataset, and save it into a 186GB <code>.npy</code> file using NumPy memory-mapped mode.</p> <pre><code>import numpy as np\nfrom tqdm.auto import tqdm\n\nimport tinytopics as tt\n\ntt.set_random_seed(42)\n\n# Generate initial data\nn, m, k = 100_000, 100_000, 20\nX, true_L, true_F = tt.generate_synthetic_data(n, m, k, avg_doc_length=256 * 256)\n\n# Save initial data\ninit_path = \"X.npy\"\nnp.save(init_path, X.cpu().numpy())\n\nsize_gb = X.nbytes / (1024**3)\nprint(f\"Memory size of X: {size_gb:.2f} GB\")\n\n# Free memory\ndel X, true_L, true_F\n\n# Create larger dataset by sampling with replacement\nn_large = 500_000\nlarge_path = \"X_large.npy\"\n\n# Create empty memory-mapped file\nshape = (n_large, m)\nlarge_size_gb = (shape[0] * shape[1] * 4) / (1024**3)  # 4 bytes per float32\nprint(f\"Expected size: {large_size_gb:.2f} GB\")\n\n# Initialize empty memory-mapped numpy array\nlarge_array = np.lib.format.open_memmap(\n    large_path,\n    mode=\"w+\",\n    dtype=np.float32,\n    shape=shape,\n)\n\n# Read and write in chunks to limit memory usage\nchunk_size = 10_000\nn_chunks = n_large // chunk_size\n\nsource_data = np.load(init_path, mmap_mode=\"r\")\n\nfor i in tqdm(range(n_chunks), desc=\"Generating chunks\"):\n    start_idx = i * chunk_size\n    end_idx = start_idx + chunk_size\n    indices = np.random.randint(0, n, size=chunk_size)\n    large_array[start_idx:end_idx] = source_data[indices]\n\n# Flush changes to disk\nlarge_array.flush()\n\n# Train using the large dataset\ndataset = tt.NumpyDiskDataset(large_path)\nmodel, losses = tt.fit_model(dataset, k=k, num_epochs=20)\n\ntt.plot_loss(losses, output_file=\"loss.png\")\n</code></pre> <p></p> <p>Each epoch now takes 5 to 6 minutes due to heavy data movement between disk, RAM, and VRAM. CPU and RAM usage are both maxed out. The peak VRAM usage is only 1.6GB, and the peak RAM usage is near 64GB.</p>"},{"location":"articles/text/","title":"Text data topic modeling","text":"<p>Tip</p> <p>Prerequisite: run text.R to get the count data and the model fitted with fastTopics for comparison:</p> <pre><code>Rscript docs/articles/static/example-text/text.R\n</code></pre> <p>To run the code from this article as a Python script:</p> <pre><code>python3 examples/text.py\n</code></pre> <p>We show a minimal example of text data topic modeling using tinytopics. The NIPS dataset contains a count matrix for 2483 research papers on 14036 terms. More details about the dataset can be found in this GitHub repo.</p>"},{"location":"articles/text/#import-tinytopics","title":"Import tinytopics","text":"<pre><code>from safetensors.numpy import load_file as load_safetensors_numpy\nfrom safetensors.torch import load_file as load_safetensors_torch\n\nimport tinytopics as tt\n</code></pre>"},{"location":"articles/text/#read-count-data","title":"Read count data","text":"<pre><code>def read_safetensors_numpy(file_path):\n    tensors = load_safetensors_numpy(file_path)\n    first_key = next(iter(tensors))\n    return tensors[first_key]\n\ndef read_safetensors_torch(file_path):\n    tensors = load_safetensors_torch(file_path)\n    first_key = next(iter(tensors))\n    return tensors[first_key]\n</code></pre> <pre><code>X = read_safetensors_torch(\"counts.safetensors\")\n\nwith open(\"terms.txt\") as file:\n    terms = [line.strip() for line in file]\n</code></pre>"},{"location":"articles/text/#fit-topic-model","title":"Fit topic model","text":"<pre><code>tt.set_random_seed(42)\n\nk = 10\nmodel, losses = tt.fit_model(X, k)\ntt.plot_loss(losses, output_file=\"loss.png\")\n</code></pre>"},{"location":"articles/text/#post-process-results","title":"Post-process results","text":"<p>We first load the L and F matrices fitted by fastTopics and then compare them with the tinytopics model. For easier visual comparison, we will try to \u201calign\u201d the topics fitted by tinytopics with those from fastTopics, and sort documents grouped by dominant topics.</p> <pre><code>L_tt = model.get_normalized_L().numpy()\nF_tt = model.get_normalized_F().numpy()\n\nL_ft = read_safetensors_numpy(\"L_fastTopics.safetensors\")\nF_ft = read_safetensors_numpy(\"F_fastTopics.safetensors\")\n\naligned_indices = tt.align_topics(F_ft, F_tt)\nF_aligned_tt = F_tt[aligned_indices]\nL_aligned_tt = L_tt[:, aligned_indices]\n\nsorted_indices_ft = tt.sort_documents(L_ft)\nL_sorted_ft = L_ft[sorted_indices_ft]\nsorted_indices_tt = tt.sort_documents(L_aligned_tt)\nL_sorted_tt = L_aligned_tt[sorted_indices_tt]\n</code></pre>"},{"location":"articles/text/#visualize-results","title":"Visualize results","text":"<p>Use Structure plot to check the document-topic distributions:</p> <pre><code>tt.plot_structure(\n    L_sorted_ft,\n    title=\"fastTopics document-topic distributions (sorted)\",\n    output_file=\"L-fastTopics.png\",\n)\n</code></pre> <p></p> <pre><code>tt.plot_structure(\n    L_sorted_tt,\n    title=\"tinytopics document-topic distributions (sorted and aligned)\",\n    output_file=\"L-tinytopics.png\",\n)\n</code></pre> <p></p> <p>Plot the probability of top 15 terms in each topic from both models to inspect their concordance:</p> <pre><code>tt.plot_top_terms(\n    F_ft,\n    n_top_terms=15,\n    term_names = terms,\n    title=\"fastTopics top terms per topic\",\n    output_file=\"F-top-terms-fastTopics.png\",\n)\n</code></pre> <p></p> <pre><code>tt.plot_top_terms(\n    F_aligned_tt,\n    n_top_terms=15,\n    term_names = terms,\n    title=\"tinytopics top terms per topic (aligned)\",\n    output_file=\"F-top-terms-tinytopics.png\",\n)\n</code></pre> <p></p>"},{"location":"reference/colors/","title":"Colors","text":""},{"location":"reference/colors/#tinytopics.colors","title":"<code>tinytopics.colors</code>","text":""},{"location":"reference/colors/#tinytopics.colors.pal_tinytopics","title":"<code>pal_tinytopics(format='hex')</code>","text":"<pre><code>pal_tinytopics(format: Literal['hex']) -&gt; MutableSequence[str]\n</code></pre><pre><code>pal_tinytopics(format: Literal['rgb', 'lab']) -&gt; NDArray[np.float64]\n</code></pre> <p>The tinytopics 10 color palette.</p> <p>A rearranged version of the original Observable 10 palette. Reordered to align with the color ordering of the D3 Category 10 palette, also known as <code>matplotlib.cm.tab10</code>. The rearrangement aims to improve perceptual familiarity and color harmony, especially when used in a context where color interpolation is needed.</p> <p>Parameters:</p> Name Type Description Default <code>format</code> <code>ColorFormat</code> <p>Returned color format. Options are:</p> <ul> <li><code>hex</code>: Hex strings (default).</li> <li><code>rgb</code>: Array of RGB values.</li> <li><code>lab</code>: Array of CIELAB values.</li> </ul> <code>'hex'</code> <p>Returns:</p> Type Description <code>MutableSequence[str] | NDArray[float64]</code> <p>Colors in the requested format:</p> <ul> <li>If <code>format='hex'</code>, returns a list of hex color strings.</li> <li>If <code>format='rgb'</code>, returns an Nx3 numpy array of RGB values.</li> <li>If <code>format='lab'</code>, returns an Nx3 numpy array of CIELAB values.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If format is not <code>'hex'</code>, <code>'rgb'</code>, or <code>'lab'</code>.</p>"},{"location":"reference/colors/#tinytopics.colors.scale_color_tinytopics","title":"<code>scale_color_tinytopics(n)</code>","text":"<p>A tinytopics 10 color scale. If &gt; 10 colors are required, will generate an interpolated color palette based on the 10-color palette in the CIELAB color space using B-splines.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The number of colors needed.</p> required <p>Returns:</p> Type Description <code>ListedColormap</code> <p>A colormap with n colors, possibly interpolated from the 10 colors.</p>"},{"location":"reference/data/","title":"Data","text":""},{"location":"reference/data/#tinytopics.data","title":"<code>tinytopics.data</code>","text":""},{"location":"reference/data/#tinytopics.data.NumpyDiskDataset","title":"<code>NumpyDiskDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset class for loading document-term matrices from <code>.npy</code> files.</p> <p>The dataset can be initialized with either a path to a <code>.npy</code> file or a NumPy array. When a file path is provided, the data is accessed lazily using memory mapping, which is useful for handling large datasets that do not fit entirely in (CPU) memory.</p>"},{"location":"reference/data/#tinytopics.data.NumpyDiskDataset.num_terms","title":"<code>num_terms</code>  <code>property</code>","text":"<p>Return vocabulary size (number of columns).</p>"},{"location":"reference/data/#tinytopics.data.NumpyDiskDataset.__init__","title":"<code>__init__(data, indices=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | Path | ndarray</code> <p>Either path to <code>.npy</code> file (str or Path) or numpy array.</p> required <code>indices</code> <code>Sequence[int] | None</code> <p>Optional sequence of indices to use as valid indices.</p> <code>None</code>"},{"location":"reference/data/#tinytopics.data.TorchDiskDataset","title":"<code>TorchDiskDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset class for loading document-term matrices from <code>.pt</code> files.</p> <p>The dataset can be initialized with either a path to a <code>.pt</code> file or a PyTorch tensor. When a file path is provided, the data is accessed lazily using memory mapping, which is useful for handling large datasets that do not fit entirely in (CPU) memory. The input <code>.pt</code> file should contain a single tensor with document-term matrix data.</p>"},{"location":"reference/data/#tinytopics.data.TorchDiskDataset.num_terms","title":"<code>num_terms</code>  <code>property</code>","text":"<p>Return vocabulary size (number of columns).</p>"},{"location":"reference/data/#tinytopics.data.TorchDiskDataset.__init__","title":"<code>__init__(data, indices=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | Path</code> <p>Path to <code>.pt</code> file (str or Path).</p> required <code>indices</code> <code>Sequence[int] | None</code> <p>Optional sequence of indices to use as valid indices.</p> <code>None</code>"},{"location":"reference/data/#tinytopics.data.IndexTrackingDataset","title":"<code>IndexTrackingDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset wrapper that tracks indices through shuffling</p>"},{"location":"reference/fit/","title":"Fit topic models","text":""},{"location":"reference/fit/#tinytopics.fit","title":"<code>tinytopics.fit</code>","text":""},{"location":"reference/fit/#tinytopics.fit.fit_model","title":"<code>fit_model(X, k, num_epochs=200, batch_size=16, base_lr=0.01, max_lr=0.05, T_0=20, T_mult=1, weight_decay=1e-05, device=None)</code>","text":"<p>Fit topic model using sum-to-one constrained neural Poisson NMF. Supports both in-memory tensors and custom datasets.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor | Dataset</code> <p>Input data, can be:</p> <ul> <li><code>torch.Tensor</code>: In-memory document-term matrix.</li> <li><code>Dataset</code>: Custom dataset implementation.   For example, see <code>NumpyDiskDataset</code>.</li> </ul> required <code>k</code> <code>int</code> <p>Number of topics.</p> required <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Number of documents per batch.</p> <code>16</code> <code>base_lr</code> <code>float</code> <p>Minimum learning rate after annealing.</p> <code>0.01</code> <code>max_lr</code> <code>float</code> <p>Starting maximum learning rate.</p> <code>0.05</code> <code>T_0</code> <code>int</code> <p>Number of epochs until first restart.</p> <code>20</code> <code>T_mult</code> <code>int</code> <p>Factor increasing restart interval.</p> <code>1</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for AdamW optimizer.</p> <code>1e-05</code> <code>device</code> <code>device | None</code> <p>Device to run training on.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[NeuralPoissonNMF, Sequence[float]]</code> <p>Tuple containing:</p> <ul> <li>Trained NeuralPoissonNMF model.</li> <li>List of training losses per epoch.</li> </ul>"},{"location":"reference/fit/#tinytopics.fit_distributed","title":"<code>tinytopics.fit_distributed</code>","text":""},{"location":"reference/fit/#tinytopics.fit_distributed.fit_model_distributed","title":"<code>fit_model_distributed(X, k, num_epochs=200, batch_size=16, base_lr=0.01, max_lr=0.05, T_0=20, T_mult=1, weight_decay=1e-05, save_path='model.pt')</code>","text":"<p>Fit topic model using sum-to-one constrained neural Poisson NMF with distributed training. Supports multi-GPU, multiple node setups via Hugging Face Accelerate.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor | Dataset</code> <p>Input data, can be:</p> <ul> <li><code>torch.Tensor</code>: In-memory document-term matrix.</li> <li><code>Dataset</code>: Custom dataset implementation.   For example, see <code>NumpyDiskDataset</code>.</li> </ul> required <code>k</code> <code>int</code> <p>Number of topics.</p> required <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Batch size.</p> <code>16</code> <code>base_lr</code> <code>float</code> <p>Minimum learning rate after annealing.</p> <code>0.01</code> <code>max_lr</code> <code>float</code> <p>Starting maximum learning rate.</p> <code>0.05</code> <code>T_0</code> <code>int</code> <p>Cosine annealing param (epochs until first restart).</p> <code>20</code> <code>T_mult</code> <code>int</code> <p>Cosine annealing param (factor for each restart).</p> <code>1</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for AdamW optimizer.</p> <code>1e-05</code> <code>save_path</code> <code>str | None</code> <p>File path to save the trained model. If None, the model will not be saved to disk.</p> <code>'model.pt'</code> <p>Returns:</p> Type Description <code>tuple[NeuralPoissonNMF, Sequence[float]]</code> <p>Tuple containing:</p> <ul> <li>Trained NeuralPoissonNMF model.</li> <li>List of training losses per epoch.</li> </ul>"},{"location":"reference/loss/","title":"Losses","text":""},{"location":"reference/loss/#tinytopics.loss","title":"<code>tinytopics.loss</code>","text":""},{"location":"reference/loss/#tinytopics.loss.poisson_nmf_loss","title":"<code>poisson_nmf_loss(X, X_reconstructed)</code>","text":"<p>Compute the Poisson NMF loss function (negative log-likelihood).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>Original document-term matrix.</p> required <code>X_reconstructed</code> <code>Tensor</code> <p>Reconstructed matrix from the model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed Poisson NMF loss.</p>"},{"location":"reference/models/","title":"Models","text":""},{"location":"reference/models/#tinytopics.models","title":"<code>tinytopics.models</code>","text":""},{"location":"reference/models/#tinytopics.models.NeuralPoissonNMF","title":"<code>NeuralPoissonNMF</code>","text":"<p>               Bases: <code>Module</code></p>"},{"location":"reference/models/#tinytopics.models.NeuralPoissonNMF.__init__","title":"<code>__init__(n, m, k, device=None)</code>","text":"<p>Neural Poisson NMF model with sum-to-one constraints on document-topic and topic-term distributions.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of documents.</p> required <code>m</code> <code>int</code> <p>Number of terms (vocabulary size).</p> required <code>k</code> <code>int</code> <p>Number of topics.</p> required <code>device</code> <code>device | None</code> <p>Device to run the model on. Defaults to CPU.</p> <code>None</code>"},{"location":"reference/models/#tinytopics.models.NeuralPoissonNMF.forward","title":"<code>forward(doc_indices)</code>","text":"<p>Forward pass of the neural Poisson NMF model.</p> <p>Parameters:</p> Name Type Description Default <code>doc_indices</code> <code>Tensor</code> <p>Indices of documents in the batch.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed document-term matrix for the batch.</p>"},{"location":"reference/models/#tinytopics.models.NeuralPoissonNMF.get_normalized_F","title":"<code>get_normalized_F()</code>","text":"<p>Get the learned, normalized topic-term distribution matrix (F).</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized F matrix on the CPU.</p>"},{"location":"reference/models/#tinytopics.models.NeuralPoissonNMF.get_normalized_L","title":"<code>get_normalized_L()</code>","text":"<p>Get the learned, normalized document-topic distribution matrix (L).</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Normalized L matrix on the CPU.</p>"},{"location":"reference/plot/","title":"Plots","text":""},{"location":"reference/plot/#tinytopics.plot","title":"<code>tinytopics.plot</code>","text":""},{"location":"reference/plot/#tinytopics.plot.plot_loss","title":"<code>plot_loss(losses, figsize=(10, 8), dpi=300, title='Loss curve', color_palette=None, output_file=None)</code>","text":"<p>Plot the loss curve over training epochs.</p> <p>Parameters:</p> Name Type Description Default <code>losses</code> <code>Sequence[float]</code> <p>List of loss values for each epoch.</p> required <code>figsize</code> <code>tuple[int, int]</code> <p>Plot size.</p> <code>(10, 8)</code> <code>dpi</code> <code>int</code> <p>Plot resolution.</p> <code>300</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'Loss curve'</code> <code>color_palette</code> <code>Sequence[str] | str | None</code> <p>Custom color palette.</p> <code>None</code> <code>output_file</code> <code>str | None</code> <p>File path to save the plot. If None, displays the plot.</p> <code>None</code>"},{"location":"reference/plot/#tinytopics.plot.plot_structure","title":"<code>plot_structure(L_matrix, normalize_rows=False, figsize=(12, 6), dpi=300, title='Structure Plot', color_palette=None, output_file=None)</code>","text":"<p>Structure plot for visualizing document-topic distributions.</p> <p>Parameters:</p> Name Type Description Default <code>L_matrix</code> <code>ndarray</code> <p>Document-topic distribution matrix.</p> required <code>normalize_rows</code> <code>bool</code> <p>If True, normalizes each row of L_matrix to sum to 1.</p> <code>False</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Plot size.</p> <code>(12, 6)</code> <code>dpi</code> <code>int</code> <p>Plot resolution.</p> <code>300</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'Structure Plot'</code> <code>color_palette</code> <code>Sequence[str] | str | None</code> <p>Custom color palette.</p> <code>None</code> <code>output_file</code> <code>str | None</code> <p>File path to save the plot. If None, displays the plot.</p> <code>None</code>"},{"location":"reference/plot/#tinytopics.plot.plot_top_terms","title":"<code>plot_top_terms(F_matrix, n_top_terms=10, term_names=None, figsize=(10, 8), dpi=300, title='Top Terms', color_palette=None, nrows=None, ncols=None, output_file=None)</code>","text":"<p>Plot top terms for each topic in horizontal bar charts.</p> <p>Parameters:</p> Name Type Description Default <code>F_matrix</code> <code>ndarray</code> <p>Topic-term distribution matrix.</p> required <code>n_top_terms</code> <code>int</code> <p>Number of top terms to display per topic.</p> <code>10</code> <code>term_names</code> <code>Sequence[str] | None</code> <p>List of term names corresponding to indices.</p> <code>None</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Plot size.</p> <code>(10, 8)</code> <code>dpi</code> <code>int</code> <p>Plot resolution.</p> <code>300</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'Top Terms'</code> <code>color_palette</code> <code>Sequence[str] | str | None</code> <p>Custom color palette.</p> <code>None</code> <code>nrows</code> <code>int | None</code> <p>Number of rows in the subplot grid.</p> <code>None</code> <code>ncols</code> <code>int | None</code> <p>Number of columns in the subplot grid.</p> <code>None</code> <code>output_file</code> <code>str | None</code> <p>File path to save the plot. If None, displays the plot.</p> <code>None</code>"},{"location":"reference/utils/","title":"Utilities","text":""},{"location":"reference/utils/#tinytopics.utils","title":"<code>tinytopics.utils</code>","text":""},{"location":"reference/utils/#tinytopics.utils.set_random_seed","title":"<code>set_random_seed(seed)</code>","text":"<p>Set the random seed for reproducibility across Torch and NumPy.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed value.</p> required"},{"location":"reference/utils/#tinytopics.utils.generate_synthetic_data","title":"<code>generate_synthetic_data(n, m, k, avg_doc_length=1000, device=None)</code>","text":"<p>Generate synthetic document-term matrix for testing the model.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of documents.</p> required <code>m</code> <code>int</code> <p>Number of terms (vocabulary size).</p> required <code>k</code> <code>int</code> <p>Number of topics.</p> required <code>avg_doc_length</code> <code>int</code> <p>Average number of terms per document.</p> <code>1000</code> <code>device</code> <code>device</code> <p>Device to place the output tensors on.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Document-term matrix.</p> <code>ndarray</code> <p>True document-topic distribution (L).</p> <code>ndarray</code> <p>True topic-term distribution (F).</p>"},{"location":"reference/utils/#tinytopics.utils.align_topics","title":"<code>align_topics(true_F, learned_F)</code>","text":"<p>Align learned topics with true topics for visualization, using cosine similarity and linear sum assignment.</p> <p>Parameters:</p> Name Type Description Default <code>true_F</code> <code>ndarray</code> <p>Ground truth topic-term matrix.</p> required <code>learned_F</code> <code>ndarray</code> <p>Learned topic-term matrix.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Permutation of learned topics aligned with true topics.</p>"},{"location":"reference/utils/#tinytopics.utils.sort_documents","title":"<code>sort_documents(L_matrix)</code>","text":"<p>Sort documents grouped by dominant topics for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>L_matrix</code> <code>ndarray</code> <p>Document-topic distribution matrix.</p> required <p>Returns:</p> Type Description <code>Sequence[int]</code> <p>Indices of documents sorted by dominant topics.</p>"}]}